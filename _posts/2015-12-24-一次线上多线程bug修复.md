---
layout: post
title: "JVM中线程太多导致资源耗尽"
description: "其实前一天句柄不足的时候，我已经意识到程序有什么资源打开了没关闭，导致资源耗尽，第二天继续坚持这个想法，去看了jvm的线程，发现下面的现象:"
category: 技术
tags: [java, jvm, tomcat, 多线程]
---
{% include JB/setup %}

## 现象：

#### 现象1：

    [root@localhost ~]# ls
    -bash: fork: Cannot allocate memory

连执行ls命令都报无法分配内存，以为是句柄泄露，经查，确实是程序中打开的ssh连接太多，导致句柄不足，ulimit -a 看，系统的 open files 是默认的1024：

    [root@localhost ~]# ulimit -a
    core file size          (blocks, -c) 0
    data seg size           (kbytes, -d) unlimited
    scheduling priority             (-e) 0
    file size               (blocks, -f) unlimited
    pending signals                 (-i) 127447
    max locked memory       (kbytes, -l) 64
    max memory size         (kbytes, -m) unlimited
    open files                      (-n) 1024
    pipe size            (512 bytes, -p) 8
    POSIX message queues     (bytes, -q) 819200
    real-time priority              (-r) 0
    stack size              (kbytes, -s) 10240
    cpu time               (seconds, -t) unlimited
    max user processes              (-u) 127447
    virtual memory          (kbytes, -v) unlimited
    file locks                      (-x) unlimited

把open files限制改成65536，在 /etc/security/limits.conf 中添加两行：

    * soft nofile 65536
    * hard nofile 65536

重启tomcat之后开开心心的下班回家了，结果到了晚上，客户在上传文件的时候又是到100%卡住， 问题还是没有解决。

然后第二天，focus on 这个问题，必须解决它，其实前一天句柄不足的时候，我已经意识到程序有什么资源打开了没关闭，导致资源耗尽，第二天继续坚持这个想法，去看了jvm的线程，发现下面的现象：

#### 现象2：

    [root@localhost bin]# ./jstack 5371|grep "on condition"|wc -l
    22678
    
发现jvm中，处于 “waiting on condition ”状态的线程非常多：

    "pool-27-thread-440" prio=10 tid=0x00007f19390a3800 nid=0x32d8 waiting on condition [0x00007f18d6064000]
       java.lang.Thread.State: WAITING (parking)
            at sun.misc.Unsafe.park(Native Method)
            - parking to wait for  <0x000000068526e6c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
            at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
            at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
            at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
            at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:957)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
            at java.lang.Thread.run(Thread.java:662)

    "pool-27-thread-439" prio=10 tid=0x00007f193909f000 nid=0x32d6 waiting on condition [0x00007f18d6266000]
       java.lang.Thread.State: WAITING (parking)
            at sun.misc.Unsafe.park(Native Method)
            - parking to wait for  <0x000000068526e6c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
            at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
            at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
            at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
            at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:957)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:917)
            at java.lang.Thread.run(Thread.java:662)

而且最开始tomcat启动的时候只有几十个、一百个，过一段时间就会猛增500左右，还以为是某个轮询中定时触发的开了线程没关闭，结果发现这个增加的时间间隔并没有规律，然后问了同事之后，说他写的某个处理用户上传之后拷贝文件的功能用到了多线程，每次会开500个线程，真是一语点醒梦中人啊，马上和这几天遇到的现象联系起来了：**每次开500个线程，如果不释放，会一直累积，导致出发了linux系统的open files限制，我们前一天改了这个限制只治了标，没有治本，虽然linux系统的句柄数限制还未达到，但是已经达到了最大线程数的限制，所以问题还是出在程序本身。** 

    [root@localhost bin]# cat /proc/sys/kernel/threads-max
    254895

## 解决
我们马上把注意力集中在这段程序上，发现java线程池初始化之后又把另一断初始化代码赋值给了这个变量：
`pool = Executors.newFixedThreadPool(BATCH_SESSION_SIZE);`， 然后在销毁线程池的时候只销毁了当前的这个pool，最开始初始化的线程都没管了。定位到问题，我们在pool被重新初始化之前先销毁，`pool.shutdown();`，问题完美解决。

后面就是线上问题的hotfix。
