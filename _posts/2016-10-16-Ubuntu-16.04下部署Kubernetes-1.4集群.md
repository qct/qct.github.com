---
layout: post
title: "Ubuntu 16.04下部署Kubernetes 1.4集群"
description: "由于GFW的存在，使得国内用户想体验 kubernetes 1.4 最新版非常痛苦，下面说一下如何快速在GFW内体验 kubernetes 1.4   "
category: 技术
tags: [技术, kubernetes]
---
{% include JB/setup %}


由于GFW的存在，使得国内用户想体验 kubernetes 1.4 最新版非常痛苦，下面说一下如何快速在GFW内体验 kubernetes 1.4   

这篇文章基于 **Ubuntu 16.04**， centos 7 的教程有很多，可以自行google，或者参考以下两篇：

[https://segmentfault.com/a/1190000007074726](https://segmentfault.com/a/1190000007074726)   
[http://www.voidcn.com/blog/foxhound/article/p-6222386.html](http://www.voidcn.com/blog/foxhound/article/p-6222386.html)

## 环境
* 192.168.2.70 (ubuntu 16.04 master)
* 192.168.2.71 (ubuntu 16.04 slave)
* 192.168.2.72 (ubuntu 16.04 slave)

## 安装docker
三台机器分别安装好docker   

```   
sudo apt-get update
sudo apt-get install apt-transport-https ca-certificates -y
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D

echo "deb https://mirrors.ustc.edu.cn/apt.dockerproject.org/repo ubuntu-xenial main" > /etc/apt/sources.list.d/docker.list
sudo apt-get update
sudo apt-get install docker-engine -y

```

其中docker源用的是[**华中科技大学的镜像源**](https://mirrors.ustc.edu.cn/)，在国内算是速度比较快的，也有[**清华的源**](https://mirrors.tuna.tsinghua.edu.cn/)可以用。用docker官方的源也可以，比国内的源慢点，你网速快可以忽略，毕竟也不大。

其他apt源可选择的比较多，我用的阿里云的源，也有网易等等可以用。

```
deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse
```

## 安装 kubernetes
### 安装kubeadm, kubectl, kubelet, kubernetes-cni, socat
在三台机器分别安装 kubeadm, kubectl, kubelet, kubernetes-cni, socat ，deb包我已经上传到百度网盘，下载下来安装即可。   

[**deb包下载**](http://pan.baidu.com/s/1dF9N749)

```
dpkg -i kubeadm_1.5.0-alpha.0-1534-gcf7301f-00_amd64.deb  kubectl_1.4.0-00_amd64.deb  kubelet_1.4.0-00_amd64.deb  kubernetes-cni_0.3.0.1-07a8a2-00_amd64.deb  socat_1.7.3.1-1_amd64.deb
```

### 下载kubernetes所需镜像
在GFW里面，按照kubernetes官方的方法是下不到kubernetes启动所需的docker镜像的， 所以我已经把需要的镜像上传到dockerhub了，直接下载打tag就行：   
在master(192.168.2.70)上执行：

```
docker pull qct222/kube-proxy-amd64:v1.4.0
docker pull qct222/kube-discovery-amd64:1.0
docker pull qct222/kubedns-amd64:1.7
docker pull qct222/kube-scheduler-amd64:v1.4.0
docker pull qct222/kube-controller-manager-amd64:v1.4.0
docker pull qct222/kube-apiserver-amd64:v1.4.0
docker pull qct222/etcd-amd64:2.2.5
docker pull qct222/kubernetes-dashboard-amd64:v1.4.0
docker pull qct222/kube-dnsmasq-amd64:1.3
docker pull qct222/exechealthz-amd64:1.1
docker pull qct222/pause-amd64:3.0
```
打tag：

```
docker tag qct222/kube-proxy-amd64:v1.4.0 gcr.io/google_containers/kube-proxy-amd64:v1.4.0 
docker tag qct222/kube-discovery-amd64:1.0 gcr.io/google_containers/kube-discovery-amd64:1.0 
docker tag qct222/kubedns-amd64:1.7 gcr.io/google_containers/kubedns-amd64:1.7 
docker tag qct222/kube-scheduler-amd64:v1.4.0 gcr.io/google_containers/kube-scheduler-amd64:v1.4.0 
docker tag qct222/kube-controller-manager-amd64:v1.4.0 gcr.io/google_containers/kube-controller-manager-amd64:v1.4.0 
docker tag qct222/kube-apiserver-amd64:v1.4.0 gcr.io/google_containers/kube-apiserver-amd64:v1.4.0 
docker tag qct222/etcd-amd64:2.2.5 gcr.io/google_containers/etcd-amd64:2.2.5 
docker tag qct222/kubernetes-dashboard-amd64:v1.4.0 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0 
docker tag qct222/kube-dnsmasq-amd64:1.3 gcr.io/google_containers/kube-dnsmasq-amd64:1.3 
docker tag qct222/exechealthz-amd64:1.1 gcr.io/google_containers/exechealthz-amd64:1.1 
docker tag qct222/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0 
```
然后删掉多余的镜像：

```
docker rmi qct222/kube-proxy-amd64:v1.4.0 
docker rmi qct222/kube-discovery-amd64:1.0 
docker rmi qct222/kubedns-amd64:1.7 
docker rmi qct222/kube-scheduler-amd64:v1.4.0 
docker rmi qct222/kube-controller-manager-amd64:v1.4.0
docker rmi qct222/kube-apiserver-amd64:v1.4.0 
docker rmi qct222/etcd-amd64:2.2.5 
docker rmi qct222/kubernetes-dashboard-amd64:v1.4.0 
docker rmi qct222/kube-dnsmasq-amd64:1.3 
docker rmi qct222/exechealthz-amd64:1.1 
docker rmi qct222/pause-amd64:3.0 
```

其中，dashboard镜像是为了后面部署dashboard用的，如果你不部署dashboard，可以不用pull。   
其实在slave上用不了这么多镜像，既然镜像都拉到本地了，无所谓多拷贝几份，可以docker save 出来之后拷贝到两台slave机器上，然后再load。这样三台机器上的images都是一样的了。
### 安装master
在192.168.2.70上执行：
```
kubeadm init
```
过一会看到
```
<master/tokens> generated token: "f0c861.753c505740ecde4c"
<master/pki> created keys and certificates in "/etc/kubernetes/pki"
<util/kubeconfig> created "/etc/kubernetes/kubelet.conf"
<util/kubeconfig> created "/etc/kubernetes/admin.conf"
<master/apiclient> created API client configuration
<master/apiclient> created API client, waiting for the control plane to become ready
<master/apiclient> all control plane components are healthy after 61.346626 seconds
<master/apiclient> waiting for at least one node to register and become ready
<master/apiclient> first node is ready after 4.506807 seconds
<master/discovery> created essential addon: kube-discovery
<master/addons> created essential addon: kube-proxy
<master/addons> created essential addon: kube-dns

Kubernetes master initialised successfully!

You can connect any number of nodes by running:

kubeadm join --token <token> <master-ip>
```
master就安装好了，用屏幕上的提示去另外两台slave上把node加进来。   
在两台slave上分别执行：
```
kubeadm join --token <token> <master-ip>
```
注意替换 token和master-ip。   
过一会在master上查看状态：
```
root@k8s1:~# kubectl get node
NAME      STATUS    AGE
k8s1      Ready     1d
k8s2      Ready     1d
k8s3      Ready     1d
```
都是ready说明正常了。

### 初始化网络
pod直接想跨node通信，必须有overlay网络支持，kubernetes官方选择了weave，你也可以选择flannel或者calico，这里使用官方教程中的weave：    

weave的两个镜像在dockerhub中已经有了，直接执行下面命令就可以自动pull下来并启动。
```
kubectl apply -f https://git.io/weave-kube
```
在master上查看状态：
```
root@k8s1:~# kubectl get pod --namespace=kube-system|grep weave
weave-net-4tbha                     2/2       Running   6          1d
weave-net-kwmf8                     2/2       Running   7          1d
weave-net-ms65i                     2/2       Running   7          1d
```
说明网络已经OK了。

### 部署dashboard

新版dashboard做了不少改进，功能增强不少，感觉还是很有必要部署的。不是dashboard的yaml在kubernetes源码里有，位置在 `cluster/addons/dashboard/` ，取下来修改service中的端口，放一个nodeport出来，然后用下面命令部署就行。

```
kubectl create -f dashboard-service.yaml
kubectl create -f dashboard-controller.yaml
```
或者使用我修改好的文件：   

dashboard-service.yaml:  

```
# This file should be kept in sync with cluster/gce/coreos/kube-manifests/addons/dashboard/dashboard-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-dashboard
  namespace: kube-system
  labels:
    k8s-app: kubernetes-dashboard
    kubernetes.io/cluster-service: "true"
spec:
  selector:
    k8s-app: kubernetes-dashboard
  type: NodePort
  ports:
  - port: 80
    targetPort: 9090
    nodePort: 30000
```

dashboard-controller.yaml:   

```
# This file should be kept in sync with cluster/gce/coreos/kube-manifests/addons/dashboard/dashboard-controller.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubernetes-dashboard-v1.4.0
  namespace: kube-system
  labels:
    k8s-app: kubernetes-dashboard
    version: v1.4.0
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 1
  selector:
    k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
        version: v1.4.0
        kubernetes.io/cluster-service: "true"
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
    spec:
      containers:
      - name: kubernetes-dashboard
        image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0
        resources:
          # keep request = limit to keep this container in guaranteed class
          limits:
            cpu: 100m
            memory: 50Mi
          requests:
            cpu: 100m
            memory: 50Mi
        ports:
        - containerPort: 9090
        livenessProbe:
          httpGet:
            path: /
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
```
过一会看到pod状态都正常了，就可以访问了： http://192.168.2.70:30000/    

如下图：   
![kubernetes-dashboard](/assets/images/kubernetes-dashboard.png "kubernetes-dashboard")  